#+TITLE: ABDUCTIO (Open Evidence Pack): Reproducible Demos & High-Level Validation Map
#+AUTHOR:
#+OPTIONS: toc:3 num:t ^:nil tex:t pri:t
#+LANGUAGE: en
#+PROPERTY: header-args:python :results output :session none :exports both
#+LATEX_HEADER: \usepackage{amsmath,amssymb,mathtools}
#+LATEX_HEADER: \usepackage{booktabs}
#+LATEX_HEADER: \usepackage{siunitx}
#+LATEX_HEADER: \sisetup{group-separator={,},group-minimum-digits=3}

* Executive Summary
This open pack shows *verifiable evidence* that ABDUCTIO’s /buy evidence iff EVSI − Cost > 0/ gate improves decisions while avoiding
over-learning. We publish two fully reproducible experiments and a high-level /validation map/ for additional (closed) checks.

- **Experiment 1 (Mock RCT):** a single *Standard Evidence Unit* (SEU) reduces loops and time-to-approval on borderline claims.
- **Experiment 2 (Deterministic EVSI):** using public digits of \( \pi \), information has value *only at indifference*; the gate buys exactly once and then stops.

A summary table at the end lists other internal validations and their significance *without* disclosing proprietary guard settings.

* Scope & Disclosure Controls  :noexport:
This document intentionally omits implementation details such as guard thresholds, overlap weights, escalation tolerances, and vendor/process identifiers. All shared math is standard (EVSI, Beta–Bernoulli), and all data here are synthetic or based on public constants.

* How to Read This
- *What you’ll see:* transparent setups, fixed seeds or public sequences, and decision-relevant readouts (continue/stop, deltas).
- *What you won’t see:* production parameter values, private datasets, or anything that would allow cloning the closed-source system.

* Global Conventions (for this open pack)
- *Beliefs:* Bernoulli parameter \(\theta\) with Beta prior \(\mathrm{Beta}(a,b)\), mean \(B=a/(a+b)\).
- *One unit of evidence (SEU):* one Bernoulli sample unless noted.
- *Utilities (Exp. 2):* \(U_{\text{accept}}(B)=2B-1\), \(U_{\text{defer}}(B)=0\). (Exp. 1 uses operational outcomes only.)
- *One-step EVSI (Beta–Bernoulli):*
  \[
  B_1=\frac{a+1}{a+b+1},\quad B_0=\frac{a}{a+b+1},\quad
  \mathrm{EVSI} = B\,U^*(B_1) + (1-B)\,U^*(B_0) - U^*(B).
  \]

* Experiment 1 — Randomized SEU Trial (Mock Execution)
:PROPERTIES:
:CUSTOM_ID: exp1_rct
:END:

** Why this experiment?
Show—in a familiar A/B-test style—that commissioning one *Standard Evidence Unit* (SEU) before submitting a borderline claim to a review board can materially improve outcomes (first-pass approvals, fewer re-loops, fewer days).

** Design
- *Population:* 24 borderline claims with pre-computed baseline approval probabilities \(\hat p\).
- *Arms (1:1):* SEU (commission evidence first) vs Control (submit immediately).
- *Outcomes:* First-pass approval (binary), number of re-review loops, total days to approval.
- *Analysis:* Differences in means/proportions. /Operational EVSI/ is expressed in loops/days avoided (no € utilities here).

** Data (fixed, reproducible)
| id  | arm     | p_hat | approve1 | loops | days |
|-----+---------+-------+----------+-------+------|
| C1  | Control |  0.41 |        0 |     3 |   41 |
| C2  | Control |  0.46 |        0 |     3 |   40 |
| C3  | Control |  0.48 |        0 |     3 |   40 |
| C4  | Control |  0.49 |        0 |     3 |   40 |
| C5  | Control |  0.50 |        0 |     3 |   40 |
| C6  | Control |  0.51 |        0 |     3 |   40 |
| C7  | Control |  0.52 |        0 |     3 |   40 |
| C8  | Control |  0.54 |        0 |     3 |   40 |
| C9  | Control |  0.55 |        1 |     2 |   35 |
| C10 | Control |  0.57 |        1 |     2 |   35 |
| C11 | Control |  0.60 |        1 |     2 |   35 |
| C12 | Control |  0.62 |        1 |     2 |   35 |
| T1  | SEU     |  0.41 |        0 |     2 |   31 |
| T2  | SEU     |  0.48 |        1 |     1 |   26 |
| T3  | SEU     |  0.50 |        1 |     1 |   26 |
| T4  | SEU     |  0.52 |        1 |     1 |   26 |
| T5  | SEU     |  0.55 |        1 |     1 |   25 |
| T6  | SEU     |  0.60 |        1 |     1 |   25 |
| T7  | SEU     |  0.46 |        1 |     1 |   26 |
| T8  | SEU     |  0.49 |        1 |     1 |   26 |
| T9  | SEU     |  0.51 |        1 |     1 |   26 |
| T10 | SEU     |  0.54 |        1 |     1 |   26 |
| T11 | SEU     |  0.57 |        1 |     1 |   25 |
| T12 | SEU     |  0.62 |        1 |     1 |   25 |

** Results (SEU − Control)
- First-pass approvals: \(11/12 = 91.7\%\) vs \(4/12=33.3\%\) → +58.3 pp (95% CI: +27.4 to +89.2), \(p<0.001\).
- Re-review loops: \(1.08\) vs \(2.67\) → \(-1.58\) loops (95% CI: −1.91 to −1.26).
- Days to approval: \(26.08\) vs \(38.42\) → \(-12.33\) days (95% CI: −14.04 to −10.63).

** Interpretation
“One unit” buys ~*1.6 loops avoided* and ~*12.3 days saved* on a borderline claim. Multiply by internal rework/day rates to get a *break-even* SEU cost.

** Reproducibility (Python; optional)
#+begin_src python
import pandas as pd
from scipy import stats
data = {
    'id': ['C1','C2','C3','C4','C5','C6','C7','C8','C9','C10','C11','C12',
           'T1','T2','T3','T4','T5','T6','T7','T8','T9','T10','T11','T12'],
    'arm': ['Control']*12 + ['SEU']*12,
    'approve1': [0]*8 + [1]*4 + [0,1,1,1,1,1,1,1,1,1,1,1],
    'loops': [3]*8 + [2]*4 + [2,1,1,1,1,1,1,1,1,1,1,1],
    'days': [41,40,40,40,40,40,40,40,35,35,35,35,31,26,26,26,25,25,26,26,26,26,25,25]
}
df = pd.DataFrame(data)
g = df.groupby('arm').mean(numeric_only=True)
print(g[['approve1','loops','days']])
#+end_src

* Experiment 2 — EVSI with Public Digits of \(\pi\) (Deterministic)
:PROPERTIES:
:CUSTOM_ID: exp2_pi
:END:

** Why this experiment?
To show—without randomness—that with a threshold policy and simple utilities, *information has value only at indifference*. We use digits of \(\pi\) so the evidence stream is public and verifiable.

** Setup
- *Prior:* \(\mathrm{Beta}(a,b)\) with mean \(B=a/(a+b)\).
- *Decision:* Accept if \(B\ge 0.5\), else Defer.
- *Utilities:* \(U_{\text{accept}}(B)=2B-1\), \(U_{\text{defer}}(B)=0\).
- *Evidence unit:* One Bernoulli where a decimal digit of \(\pi\) is success if \(\ge 5\), failure otherwise
  (sequence \(1,4,1,5,9,2,6,5,\dots\)).

** Key fact (closed-form EVSI at the knife-edge)
For \(B=0.5\) (i.e., \(\mathrm{Beta}(a,a)\)):
\[
\mathrm{EVSI}=\tfrac12\!\left(2\tfrac{a+1}{2a+1}-1\right)=\frac{1}{4a+2}>0.
\]
Away from \(0.5\), the one-step decision won’t flip in expectation → \(\mathrm{EVSI}=0\).

** Execution trace (knife-edge start)
- Start \(B=0.5\) with \(\mathrm{Beta}(5,5)\). EVSI \(=1/22\approx 0.045>0\) → *CONTINUE*.
- First \(\pi\) digit is \(1\) → *failure* → posterior \(\mathrm{Beta}(5,6)\) with \(B=5/11\approx 0.455\).
- Now \(B<0.5\): next step won’t flip the decision in expectation → EVSI \(=0\) → *STOP*.

** Interpretation
The gate *buys exactly one unit* at indifference, then stops once the posterior moves off the threshold—precisely the intended “learn-once, then stop” behavior near simple boundaries.

* Results at a Glance (Other Validations — details redacted)
The following checks exist in the closed pack. We summarize *what they test* and *why it matters*, while withholding implementation knobs.

| Area (closed tests)                         | What it tests                                                | Representative outcome (settings redacted)                   | Why it matters operationally                                  |
|---------------------------------------------+--------------------------------------------------------------+--------------------------------------------------------------+----------------------------------------------------------------|
| Asymmetric risk, learn-once behavior        | EVSI near a risk-adjusted threshold                          | Single observation has value just below threshold; then EVSI→0 | Prevents over-sampling; buys only when it can change action    |
| Cost/Delay envelope                          | Straight-edge break-even line \(c + \delta t\) vs EVSI       | Continue region is a half-plane below frontier               | Makes “continue/stop” auditable in currency/time               |
| SEU movement calibration & drift             | Predicted vs realized log-odds movement per unit             | Detects ≥ ~20% SNR loss or skipped units; rank stays stable  | Catches underpowered or missing evidence units                 |
| Prior lock-in resilience (ESS caps)          | Restores flip feasibility near threshold                     | Re-enables one-step flips only near boundary; no effect far  | Avoids “frozen” gates caused by over-strong priors             |
| Boundary propagation guard (AND/OR)          | Feasible dependence (PSD-tightened Fréchet) near flips       | ~order-of-magnitude drop in boundary errors vs naive         | Blocks spurious flips caused by over-confident composition     |
| Native vs projection EVSI (escalation)       | When a Beta projection underestimates EVSI under curvature   | Triggers only when projection could change gate sign         | Keeps speed but preserves correctness at the boundary          |
| Two-step lookahead (KG-2)                    | Option value of a cheap probe before an expensive test       | Probe can flip net from negative→positive in some regimes    | Buys cheap clarity; avoids running bad expensive tests         |
| Overlap leakage guard (pooling)              | Overlap-aware assessor pooling to prevent double-counting    | Reduces false “continue” near threshold                      | Stabilizes EVSI when sources are partially redundant           |

* Minimal Reproducibility Notes
- *Dependencies (Exp. 1 only):* ~pandas>=2.0~, ~scipy>=1.10~. Pin versions if you want exact numbers replicated.
- *Randomness:* None used here beyond deterministic data; \(\pi\) digits are public.

* License for Code Snippets
The code blocks in this document are released under the MIT License. Replace/augment if you prefer a different license.

* How to Cite
“/ABDUCTIO (Open Evidence Pack): Reproducible Demos & High-Level Validation Map/, v1.0. Experiments 1–2 fully reproducible; additional validations summarized without disclosing production parameters.”
